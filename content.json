{"meta":{"title":"Xin","subtitle":null,"description":null,"author":"Xin Mei","url":"http://www.followeye.com"},"pages":[{"title":"分类","date":"2017-03-23T06:00:15.000Z","updated":"2017-03-23T07:57:38.666Z","comments":true,"path":"categories/index.html","permalink":"http://www.followeye.com/categories/index.html","excerpt":"","text":""},{"title":"标签","date":"2017-03-10T10:06:35.000Z","updated":"2017-03-10T10:14:48.675Z","comments":true,"path":"tags/index.html","permalink":"http://www.followeye.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"数据处理系统重构","slug":"数据处理系统的重构","date":"2017-03-29T14:35:26.000Z","updated":"2017-03-31T03:51:58.222Z","comments":true,"path":"2017/03/29/数据处理系统的重构/","link":"","permalink":"http://www.followeye.com/2017/03/29/数据处理系统的重构/","excerpt":"","text":"数据处理系统是一个复杂庞大的系统，涵盖了数据采集、清洗、计算、加载，数据挖掘，数据可视化等环节，每个环节都是一个复杂的系统，也有各种各样的技术实现，整个系统由各个环节串联而成，任何环节都可能影响整个系统的可靠性，稳定性和处理能力，所以选择合适的技术和方案很重要。 重构原因原有的数据处理系统大概架构，如图 1。其中：Amazon S3: Amazon Simple Storage Service, 亚马逊云存储服务。Amazon Redshift: 亚马逊基于PostgreSQL开发的数据仓库服务。 如图1 所示，整个系统主要有采集、清洗、计算，存储，上层应用等主要环节，主要基于Redshift做离线计算。这个系统主要的问题: 数据处理速度慢随着业务增长，数据量越来越大，基于Redshift的计算越来越慢，一个job跑完花费几个小时，报表生成滞后严重，影响相关人员的使用。 监控不完善监控不足，导致数据异常后，不能及时发现，发现了也很难排查和定位问题，需要很高的人力成本。 数据时效性低由于整个系统没有实时计算，导致数据的时效性低，不能实时了解线上业务运行状况。 综上原因，老的数据处理系统，已经不能满足业务发展需求，因此开始了整个系统的重构。 新数据处理系统新的数据处理系统主要引入了Kinesis，Spark等相关技术，同时开发了新的数据可视化系统。新数据处理系统包括多个业务功能，整体数据流架构如图2，其中：Amazon Kinesis Stream/Firehose: 亚马逊的Kafka，又在功能上进行了扩展。Kstash/Ksplit/Kupload: 实时数据清洗、处理、分发应用。 如图所示，整个大数据处理过程包括数据源、数据通道、数据计算处理、数据存储、应用等环节。各个环节都具有弹性，出现性能瓶颈时，很容易扩展。数据流向的各环节有监控，出现异常能及时报警。整个系统的计算有两种：离线计算、实时计算。 下面说明下大数据处理系统各个阶段的实现及原因。 数据采集 本公司的游戏各个产品主要面向欧美和亚洲用户，横跨多个时区，为了节省成本、提高效率，公司的各个系统偏向使用亚马逊提供的云服务，包括服务器，数据库，存储等等。 由于业务越来越多，数据源也越来越多，包括老的游戏gameserver记录的数据，API server采集的客户端或SDK的数据，第三方平台的发送的数据等等，数据位置也分散在世界各地，数据源的复杂性，使得实现快速、稳定的数据采集系统有很大的挑战。 经过技术调研，最终选择Amazon Kinesis作为数据通道，将各种数据源统一接入，在各个数据源处，部署Kinesis Agent，自动将数据采集到Kinesis中。Amazon Kinesis 实时、可靠、易用、成本低，可随时根据需要动态调节数据流的吞吐量，使用它，使整个采集系统更可靠，容易维护。 数据处理与加载 老系统是基于SQL的，各个指标的计算逻辑都是使用SQL实现的，使用MapReduce或其他方式来重新计算各个指标，工作量实在太大。所以首选支持SQL的大数据方案。经过比较，最终选择使用Spark，它提供了SQL计算引擎，Spark SQL，只需较少的代价，就可替代老的SQL计算逻辑。同时，Spark 还提供了Spark Streaming、Spark ML，只需一种框架就能实现实时计算、机器学习等功能，大大降低了使用成本。 Kinesis中的数据多种多样，有结构化的数据，也有非结构化的数据。对结构化的数据，可以直接供spark计算。对于非结构化的数据，先经过清洗、处理，生成结构化的数据，再供spark计算，以便减轻spark集群的压力，提高计算速度。 经过计算处理后的数据，按不同的业务需求，加载到不同存储中，比如每日报表、实时指标加载到Redshift，线上应用实时日志加载到Elasticsearch中，用户画像系统数据加载到Hbase中。 数据流程监控 监控功能涵盖了数据采集、计算、加载各个环节，监控指标包括各个游戏数据量、指标的计算和加载状态、job状态、时长等，一旦有异常及时报警。 数据应用 老系统使用第三方BI工具，不够灵活。所以对每日报表和实时指标，重新开发了一套完善的可视化系统。线上实时日志检索和分析，使用Kibana。用户画像产生的数据，供运营系统使用。 难点开发新系统遇到了很多的难点，其中最大的是Spark的使用。刚开始的Spark 程序运行很慢，不能满足需求，逐步优化，最终，离线处理时间降到了一小时内。 总结新大数据处理系统经过近一年的开发、部署、测试，完全替代了老的系统，并稳定运行。新系统带来了很大的改进: Spark 集群规模不到Redshift集群一半，离线计算速度却提高了5倍以上 实时数据帮助实时监控线上产品运行状况 更加完善的数据体系让产品运营更加方便、更加高效、更加精准","categories":[{"name":"bigdata","slug":"bigdata","permalink":"http://www.followeye.com/categories/bigdata/"},{"name":"architecture","slug":"bigdata/architecture","permalink":"http://www.followeye.com/categories/bigdata/architecture/"}],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://www.followeye.com/tags/Elasticsearch/"},{"name":"Hbase","slug":"Hbase","permalink":"http://www.followeye.com/tags/Hbase/"},{"name":"bigdata","slug":"bigdata","permalink":"http://www.followeye.com/tags/bigdata/"},{"name":"architecture","slug":"architecture","permalink":"http://www.followeye.com/tags/architecture/"},{"name":"Spark SQL","slug":"Spark-SQL","permalink":"http://www.followeye.com/tags/Spark-SQL/"},{"name":"Spark Streaming","slug":"Spark-Streaming","permalink":"http://www.followeye.com/tags/Spark-Streaming/"},{"name":"Spark ML","slug":"Spark-ML","permalink":"http://www.followeye.com/tags/Spark-ML/"},{"name":"Amazon Kinesis","slug":"Amazon-Kinesis","permalink":"http://www.followeye.com/tags/Amazon-Kinesis/"},{"name":"Amazon Redshift","slug":"Amazon-Redshift","permalink":"http://www.followeye.com/tags/Amazon-Redshift/"}]},{"title":"《Elasticsearch与Hbase比较》","slug":"Elasticsearch与Hbase比较","date":"2017-03-26T15:19:31.000Z","updated":"2017-03-27T04:38:39.946Z","comments":true,"path":"2017/03/26/Elasticsearch与Hbase比较/","link":"","permalink":"http://www.followeye.com/2017/03/26/Elasticsearch与Hbase比较/","excerpt":"","text":"Elasticsearch和Hbase都是非常流行的大数据存储方案，有相似也有很大不同，本文对二者做个比较全面的比较。 介绍Elasticsearch是一个近实时的搜索引擎，对document进行索引和搜索，索引包括基本的CRUD操作，搜索基于自己的Query DSL，能执行复杂的查询，此外还有聚合功能等等。它的数据模型是：索引，类型，文档。 Hbase是运行与hadoop之上的Nosql数据库，它的数据模型：表、行、列族、Cells，它对数据模型的主要操作是Get, Put, Scan, 和 Delete。 比较对比列表 特性 Elasticsearch Hbase 说明 扩展性 可扩展 可扩展 部署难易 简单 复杂 Hbase依赖hadoop，部署复杂。 高可用 是 是 二者架构都实现了高可用。 数据量 海量数据 海量数据 并发能力 高并发 高并发 一致性 最终一致性 强一致性 Elasticsearch有三种一致性方案。 访问方式 Java API，Restful API Java API，Restful API，Thrift Server 编程语言 多种 多种 实现语言 Java Java SQL支持 第三方工具 第三方工具 MapReduce 不支持 支持 分区方式 sharding sharding Elasticsearch索引会分成不同的分片，Hbase的表会划分成region。 schema schema-free schema-free 二者都易增减字段，但是es的字段带类型，类型确定后，不能更改。 多索引 是 否 Elasticsearch每个字段都被索引。 写入速度 不快 非常快 查询速度 很快 get、限定rowkey的查询很快 查询功能 查询功能强 简单查询 写能力Elasticsearch通过api提供了普通写入和bulk写入两种方式，可以通过修改配置、增加节点方式提高写入速度，可达几千，几万documents/s, 但在海量数据下，仍显太慢。同样集群，es写入速度远比Hbase慢。 Hbase写入速度快，也提供了bulkload等多种方法导入数据，提高了海量数据写入速度。 读能力Elasticsearch中各个字段都被索引，不仅get操作快，按多个字段的查询操作也很快。 Hbase的get操作非常快， 如果不限定rowkey，scan非常慢。 总结Elasticsearch 适合应用于流式写入，频繁多字段查询、分析的场景。不适合频繁大量写入、更新数据的场景。 Hbase 适合频繁大量数据写入、更新，频繁的key-value查询的场景。不适合多字段查询的场景。 ...","categories":[{"name":"bigdata","slug":"bigdata","permalink":"http://www.followeye.com/categories/bigdata/"},{"name":"Elasticsearch","slug":"bigdata/Elasticsearch","permalink":"http://www.followeye.com/categories/bigdata/Elasticsearch/"},{"name":"Hbase","slug":"bigdata/Elasticsearch/Hbase","permalink":"http://www.followeye.com/categories/bigdata/Elasticsearch/Hbase/"}],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://www.followeye.com/tags/Elasticsearch/"},{"name":"Hbase","slug":"Hbase","permalink":"http://www.followeye.com/tags/Hbase/"},{"name":"对比","slug":"对比","permalink":"http://www.followeye.com/tags/对比/"},{"name":"bigdata","slug":"bigdata","permalink":"http://www.followeye.com/tags/bigdata/"}]},{"title":"《Elasticsearch从开始到优化：重要配置》","slug":"elasticsearch从开始到优化：重要配置","date":"2017-03-19T13:46:25.000Z","updated":"2017-03-24T05:41:56.811Z","comments":true,"path":"2017/03/19/elasticsearch从开始到优化：重要配置/","link":"","permalink":"http://www.followeye.com/2017/03/19/elasticsearch从开始到优化：重要配置/","excerpt":"","text":"Elasticsearch启动时会检查一些很重要的设置是否配置，如果没配置，可能会出现无法预料的问题。 File Descriptors文件描述符设置。使用Elasticsearch 的zip或tar.gz安装包时，永久修改修改max file descriptors: sudo vim /etc/security/limits.conf 12* soft nofile 655360* hard nofile 655360 新打开的session里生效。 Number of threads用户进程数设置。使用Elasticsearch 的zip或tar.gz安装包时，永久修改修改max user processes: sudo vim /etc/security/limits.conf 12* soft nproc 50960* hard nproc 50960 有的系统需要： sudo vim /etc/security/limits.d/20-nproc.conf 1* soft nproc 50960 新打开的session里生效。 Virtual memory虚拟内存设置。Elasticsearch默认使用mmapfs、sniofs 两种方式存储索引。也可以修改配置index.store.type确定使用任意一种。操作系统默认的最大mmap数比较低，可能会导致内存异常。永久修改修改max_map_count: sudo vim /etc/sysctl.conf， 1vm.max_map_count=2621440 然后执行命令： sudo sysctl -p heap size设置设置heap size最小值(Xms)，heap size最大值(Xmx), 通过修改配置文件，vim elasticsearch-5.1.2/config/jvm.options 12-Xms2g-Xmx2g 堆设置很好的经验法则： 最小heap size(Xms) 和 最大heap size(Xmx)相等 最大heap size(Xmx)设置为50%物理内存，剩下50%作为文件系统缓存。 不超过32GB。堆大小在某个cutoff(将近32GB)范围内时，JVM使用compressed oops技术来优化内存、带宽、缓存使用。所以最大heap size(Xmx)不要超过这个cutoff。Elasticsearch的log中如果有下面类似的log，表明Xms没超过这个cutoff。 1heap size [1.9gb], compressed ordinary object pointers [true] 更好的设置是，heap size不要超过JVM使用zero-based compressed oops技术时的cutoff。不同的系统，cutoff不相同。大部分系统，26GB的最大heap size(Xmx)是安全的，有些系统30G也可以。带上 -XX:+UnlockDiagnosticVMOptions -XX:+PrintCompressedOopsMode这两个JVM options启动Elasticsearch时，下面的log则表明启用了zero-based compressed oops技术： 1heap address: 0x000000011be00000, size: 27648 MB, zero based Compressed Oops 下面的则表明未启用： 1heap address: 0x0000000118400000, size: 28672 MB, Compressed Oops with base: 0x00000001183ff000 Java中的对象分配在堆中，由指针引用。这些指向java对象的指针叫做Ordinary object pointers(oop)，他们的大小实际就是CPU的本地字长，32bit或64bit，这 32 bits 或 64 bits 的指针的值，是堆中确定的byte位置。 所以32-bit的系统中，oop最多只能指向2^32 byte, 即heap size最大只能4GB，而64-bit系统，heap size大的多。64位的指针本身更大，占用空间更多，同一个应用在64-bit系统使用的堆空间大概是32-bit系统的1.5倍。除了更消耗内存，更糟糕的是，更长的指针在主存和各种缓存(LLC, L1等等)间移动时，消耗更多的带宽。 32 bits 的短指针来引用的内存空间不够，64 bits的长指针会带来性能损耗，怎么办呢？JVM使用了Compressed Ordinary Object Pointers(compressed oops)的技术。这种 32 bits 指针指向的不再是堆中byte 位置，而是对象偏移量。这意味着 32 bits 的指针可以指向2^32个对象，大约32GB大小。一旦超过32G，compressed oops会变成oop，所以heap size不要超过32G。 Disable swapping设置swapping(内存交换)会导致JVM heap被交换到磁盘上，导致很差的性能和Elasticsearch节点的不稳定。应该不惜一切代价禁止swapping。禁止swapping的一种方法就是锁住JVM内存 锁内存修改配置文件，vim elasticsearch-5.1.2/config/elasticsearch.yml 1bootstrap.memory_lock: true 赋予运行Elasticsearch的用户(本文用户是hadoop)锁内存权限： sudo vim /etc/security/limits.conf 123# allow user &apos;hadoop&apos; mlockallhadoop soft memlock unlimitedhadoop hard memlock unlimited ...","categories":[{"name":"bigdata","slug":"bigdata","permalink":"http://www.followeye.com/categories/bigdata/"},{"name":"Elasticsearch","slug":"bigdata/Elasticsearch","permalink":"http://www.followeye.com/categories/bigdata/Elasticsearch/"}],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://www.followeye.com/tags/Elasticsearch/"},{"name":"bigdata","slug":"bigdata","permalink":"http://www.followeye.com/tags/bigdata/"},{"name":"optimization","slug":"optimization","permalink":"http://www.followeye.com/tags/optimization/"},{"name":"重要配置","slug":"重要配置","permalink":"http://www.followeye.com/tags/重要配置/"}]},{"title":"《Elasticsearch从开始到优化：性能优化》","slug":"elasticsearch从开始到优化：性能优化","date":"2017-03-16T12:46:25.000Z","updated":"2017-03-24T05:44:39.842Z","comments":true,"path":"2017/03/16/elasticsearch从开始到优化：性能优化/","link":"","permalink":"http://www.followeye.com/2017/03/16/elasticsearch从开始到优化：性能优化/","excerpt":"","text":"Elasticsearch默认配置提供了良好的开箱即用的体验。但在不同的场景下，也有许多优化方法来提高Elasticsearch的性能。 通用建议不要返回大的查询结果集如果需要，使用 scroll 接口。 避免大文档Elasticsearch和Lucene都有文档大小限制。大的文档会造成更多的网络、内存、磁盘的消耗，影响索引和搜索速度。 避免稀疏Elasticsearch依赖Lucene来进行索引和存储数据，Lucene背后的数据结构在密集数据时工作的最好。尽量同一个索引下文档结构相同，避免稀疏，相关建议： 1.同一个索引里避免放入不相关的文档 2.标准化文档结构 3.同一个索引避免不同类型 4.对稀疏字段禁用 norms 和 doc_values用于过滤不需要需要打分的字段，禁用 norms。不被用于排序和聚合的字段，禁用 doc_values。 索引速度优化使用bulk请求bulk请求性能远高于单文档索引请求。并不是每次bulk越多文档越好，可对节点进行测试，找出每次最佳bulk文档数。 使用多进程/线程向Elasticsearch发送数据单线程bulk请求不太可能达到集群最大索引能力。所以使用多进程/线程，可以测试找出最优进程/线程数量。 增加刷新间隔Elasticsearch 默认刷新间隔index.refresh_interval是1s，意味着每秒都会强制Elasticsearch创建segment，以便索引的文档能够被搜索到。增加刷新间隔降低创建segment频率，使写入磁盘的segment更大，并且减小未来segmeng合并的压力。 禁止刷新和复制当需要大量加载数据时，禁止刷新和复制，可以数倍提供索引速度。设置刷新间隔index.refresh_interval为-1，副本数index.number_of_replicas为0，便可禁止刷新和复制。禁止复制时，副本数为0，可能导致数据丢失。数据加载完成后，便可将两个参数调整为原来的值。 禁止swappingswapping(内存交换)会导致JVM heap被交换到磁盘上，导致很差的性能和节点稳定性。应该不惜一切代价禁止swapping。 确保有空闲内存用于文件系统缓存文件系统缓存用于缓冲IO操作，确保至少一半内存用于文件系统缓存。 使用自动生成的id索引一个文档时显示指定id，Elasticsearch会检查分片内是否有相同的id， 索引越大，这个代价就越高。 使用更快的硬件使用更快的磁盘，使用本地存储。 索引缓冲区大小确保重索引操作的shard的缓冲区indices.memory.index_buffer_size足够大。 搜索速度优化确保有空闲内存用于文件系统缓Elasticsearch严重依赖文件系统缓存来保证快速搜索。 使用更快的硬件如果搜索瓶颈在I/O，那就分配更大的文件系统缓存，使用更快的存储。如果搜索瓶颈在CPU，那就使用更快的cpu。 文档建模使用合理的文档模型，避免join操作，嵌套使查询慢数倍，父子关系使查询慢百倍。 预热文件系统缓存通过配置index.store.preload 告诉操作系统哪些文件需要被加载到内存中。谨慎使用，避免出现文件太大，文件系统缓存不够加载的情况，会导致搜索变慢。 查询语法优化使用过滤上下文filter context，常用的过滤器会被Elasticsearch缓存，提高性能。 使用routingElasticsearch默认使用文档的id进行routing，将文档映射到某个shard上。索引时可以根据文档的某个字段进行routing，搜索时指定routing便可到相应的shard上搜索，提高搜索速度，同时大大降低集群资源的使用。 分索引索引越来越大，shard也越来越大，查询速度越来越慢。Elasticsearch不提供shard split的功能，可以创建新索引，保证单个索引不会太大。 ...","categories":[{"name":"bigdata","slug":"bigdata","permalink":"http://www.followeye.com/categories/bigdata/"},{"name":"Elasticsearch","slug":"bigdata/Elasticsearch","permalink":"http://www.followeye.com/categories/bigdata/Elasticsearch/"}],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://www.followeye.com/tags/Elasticsearch/"},{"name":"bigdata","slug":"bigdata","permalink":"http://www.followeye.com/tags/bigdata/"},{"name":"性能优化","slug":"性能优化","permalink":"http://www.followeye.com/tags/性能优化/"},{"name":"optimization","slug":"optimization","permalink":"http://www.followeye.com/tags/optimization/"}]}]}